# MediMind ğŸ¥ğŸ‘¨â€âš•ï¸ - End-to-End Medical Chatbot Using Generative AI

![MediMind Banner](https://img.shields.io/badge/MediMind-Medical%20Chatbot-blue)
[![Python 3.10](https://img.shields.io/badge/Python-3.10-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ“‘ Table of Contents
- [About The Project](#about-the-project)
- [Features](#features)
- [Project Structure](#project-structure)
- [Tech Stack](#tech-stack)
- [Getting Started](#getting-started)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
- [Usage](#usage)
- [Code Snippets](#code-snippets)
- [How It Works](#how-it-works)
- [Contributing](#contributing)
- [License](#license)
- [Contact](#contact)

## ğŸ” About The Project

MediMind is an end-to-end medical chatbot powered by Generative AI that provides accurate medical information based on the data it has been trained on. The chatbot leverages RAG (Retrieval Augmented Generation) to provide factual medical information by combining the power of large language models with retrievable medical knowledge. ğŸ§ ğŸ’Š
## Interactive Asthetic Display

![Plot](Display.png)
## âœ¨ Features

- ğŸ¤– Intelligent medical chatbot powered by GPT
- ğŸ“š PDF document processing for medical knowledge
- ğŸ” Vector search for accurate information retrieval
- ğŸ§ª RAG architecture for fact-based responses
- ğŸ’» User-friendly web interface
- ğŸ”„ Real-time conversational experience
- ğŸ“Š Contextual understanding of medical queries
- ğŸ”’ Local deployment for data privacy
- ğŸ“± Responsive design for all devices

## ğŸ“ Project Structure

```
MediMind/
â”œâ”€â”€ Data/                # Directory containing medical PDF documents ğŸ“‘
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ helper.py        # Helper functions for document loading and processing ğŸ› ï¸
â”‚   â””â”€â”€ prompt.py        # System prompts for the LLM ğŸ’¬
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ chat.html        # HTML template for the chat interface ğŸ–¥ï¸
â”œâ”€â”€ app.py               # Main Flask application ğŸŒ
â”œâ”€â”€ requirements.txt     # Project dependencies ğŸ“‹
â”œâ”€â”€ setup.py             # Setup script for the package ğŸ“¦
â”œâ”€â”€ store_index.py       # Script to create and populate the vector index ğŸ—‚ï¸
â””â”€â”€ README.md            # Project documentation ğŸ“
```

## ğŸ› ï¸ Tech Stack

- **Python 3.10** ğŸ - Programming Language
- **Flask** ğŸŒ¶ï¸ - Web framework
- **LangChain** â›“ï¸ - Framework for LLM applications
- **OpenAI** ğŸ§  - GPT model for natural language processing
- **Pinecone** ğŸŒ² - Vector database for storing embeddings
- **HuggingFace** ğŸ¤— - Embeddings model (sentence-transformers)
- **HTML/CSS/JS** ğŸ¨ - Frontend for the chat interface
- **Conda** ğŸ - Environment management

## ğŸš€ Getting Started

Follow these instructions to set up and run the project on your local machine.

### Prerequisites

- Python 3.10 or higher ğŸ
- Conda (recommended for environment management) ğŸ“¦
- Pinecone API Key ğŸ”‘
- OpenAI API Key ğŸ”‘
- Medical PDF documents ğŸ“š

### Installation

1. **Clone the repository** ğŸ“¥
   ```bash
   git clone https://github.com/yourusername/MediMind.git
   cd MediMind
   ```

2. **Create and activate Conda environment** ğŸŒ
   ```bash
   conda create -n medibot python=3.10 -y
   conda activate medibot
   ```

3. **Install required packages** ğŸ“¦
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables** ğŸ”
   
   Create a `.env` file in the root directory and add your API keys:
   ```
   PINECONE_API_KEY="your_pinecone_api_key"
   OPENAI_API_KEY="your_openai_api_key"
   ```

5. **Prepare your medical PDF documents** ğŸ“‘
   
   Place your medical PDF documents in the `Data/` directory.

6. **Create and populate the vector index** ğŸ—‚ï¸
   ```bash
   python store_index.py
   ```
   
   This will:
   - Load the PDF documents from the `Data/` directory ğŸ“„
   - Split them into text chunks ğŸ“
   - Generate embeddings for each chunk ğŸ§®
   - Create a Pinecone index named "medicalbot" ğŸŒ²
   - Store the embeddings in the Pinecone index ğŸ’¾

7. **Run the application** ğŸƒâ€â™‚ï¸
   ```bash
   python3 app.py
   ```

8. **Access the chat interface** ğŸ’¬
   
   Open your web browser and go to:
   ```
   http://localhost:8080
   ```

## ğŸ’¬ Usage

1. Access the web interface at `http://localhost:8080` ğŸŒ
2. Type your medical questions in the chat input ğŸ”
3. Get responses based on the medical documents provided âœ…

Example questions you can ask:
- "What are the symptoms of diabetes?" ğŸ©º
- "How does hypertension affect the heart?" â¤ï¸
- "What are the side effects of aspirin?" ğŸ’Š
- "How should I manage my chronic pain?" ğŸ¤•

The chatbot uses retrieval-augmented generation to provide accurate answers based on the medical documents you've provided in the `Data/` directory. ğŸ“šğŸ¤–

## ğŸ’» Code Snippets

### 1. Loading and Processing PDF Documents ğŸ“‘

```python
# From helper.py
def load_pdf_file(data):
    loader = DirectoryLoader(data,
                            glob="*.pdf",
                            loader_cls=PyPDFLoader)

    documents = loader.load()
    return documents

def text_split(extracted_data):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)
    text_chunks = text_splitter.split_documents(extracted_data)
    return text_chunks
```

### 2. Creating Embeddings ğŸ§®

```python
# From helper.py
def download_hugging_face_embeddings():
    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    return embeddings
```

### 3. Setting Up the Flask Application ğŸŒ

```python
# From app.py
app = Flask(__name__)

load_dotenv()

PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

# Set up retriever and language model
embeddings = download_hugging_face_embeddings()
docsearch = PineconeVectorStore.from_existing_index(
    index_name="medicalbot",
    embedding=embeddings
)
retriever = docsearch.as_retriever(search_type="similarity", search_kwargs={"k":3})
```

### 4. Creating the RAG Chain â›“ï¸

```python
# From app.py
llm = OpenAI(temperature=0.4, max_tokens=500)
prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{input}"),
])

question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)
```

### 5. Processing Chat Requests ğŸ’¬

```python
# From app.py
@app.route("/get", methods=["GET", "POST"])
def chat():
    msg = request.form["msg"]
    input = msg
    print(input)
    response = rag_chain.invoke({"input": msg})
    print("Response : ", response["answer"])
    return str(response["answer"])
```

## ğŸ§ª How It Works

1. **Document Processing** ğŸ“„:
   - Medical PDFs are loaded and processed through `DirectoryLoader` and `PyPDFLoader`
   - Documents are split into manageable chunks using `RecursiveCharacterTextSplitter`
   - Text chunks are converted into embeddings using HuggingFace models (384 dimensions)

2. **Vector Database Setup** ğŸ—‚ï¸:
   - Pinecone serverless instance is created in AWS
   - Embeddings are stored with document references for retrieval
   - Vector similarity search is used to find relevant information

3. **Retrieval System** ğŸ”:
   - User queries are converted to embeddings using the same model
   - Similar document chunks are retrieved from Pinecone using cosine similarity
   - The top 3 most relevant chunks are selected (`k=3`)

4. **Response Generation** ğŸ¤–:
   - Retrieved information is fed to the LLM (OpenAI)
   - System prompt guides the LLM to use the context for answers
   - Temperature setting of 0.4 balances creativity and accuracy
   - The LLM generates a concise response (max 3 sentences)
   - Response is returned to the user through the Flask web interface

5. **User Experience** ğŸ‘¨â€ğŸ’»:
   - Clean and intuitive chat interface
   - Real-time interaction with the medical knowledge base
   - Responses are focused on medical knowledge from the provided documents

## ğŸ‘ Contributing

Contributions are what make the open-source community such an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**. ğŸ™

1. Fork the Project ğŸ´
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`) ğŸŒ¿
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`) ğŸ’¾
4. Push to the Branch (`git push origin feature/AmazingFeature`) ğŸš€
5. Open a Pull Request ğŸ“¬

## ğŸ“„ License

Distributed under the MIT License. See `LICENSE` for more information. âš–ï¸

---

â­ Star this repo if you found it useful! â­
